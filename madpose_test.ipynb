{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e6add6-e8f0-4af0-979d-df767a152ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import madpose\n",
    "#import poselib\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import madpose\n",
    "from madpose.utils import compute_pose_error, get_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92c3f26-8c50-4b88-ae95-8832e0656b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure RANSAC options\n",
    "options = madpose.HybridLORansacOptions()\n",
    "options.min_num_iterations = 100\n",
    "options.max_num_iterations = 1000\n",
    "options.success_probability = 0.9999\n",
    "options.random_seed = 0\n",
    "options.final_least_squares = True\n",
    "options.threshold_multiplier = 5.0\n",
    "# Set reprojection (e.g., 4px) and epipolar error thresholds\n",
    "options.squared_inlier_thresholds = [4.0**2, 4.0**2] \n",
    "options.data_type_weights = [1.0, 1.0]\n",
    "\n",
    "# 2. Configure Estimator settings\n",
    "est_config = madpose.EstimatorConfig()\n",
    "est_config.min_depth_constraint = True # Ensures positive depth\n",
    "est_config.use_shift = True            # Models depth shift\n",
    "est_config.ceres_num_threads = 8       # Set to number of physical CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f24893-2f27-4a78-a4e3-a754f0b51eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'depth_pp1_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m depth_1_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sample_path, info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_1_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     23\u001b[0m depth_map0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(depth_0_file)\n\u001b[0;32m---> 24\u001b[0m depth_map1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[43mdepth_pp1_file\u001b[49m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Query the depth priors of the keypoints\u001b[39;00m\n\u001b[1;32m     27\u001b[0m depth0 \u001b[38;5;241m=\u001b[39m get_depths(image0, depth_map0, mkpts0)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'depth_pp1_file' is not defined"
     ]
    }
   ],
   "source": [
    "sample_path = \"examples/image_pairs/1_eth3d\"\n",
    "# Read the image pair\n",
    "image0 = cv2.imread(os.path.join(sample_path, \"image0.png\"))\n",
    "image1 = cv2.imread(os.path.join(sample_path, \"image1.png\"))\n",
    "\n",
    "# Read info\n",
    "with open(os.path.join(sample_path, \"info.json\")) as f:\n",
    "    info = json.load(f)\n",
    "\n",
    "# Load camera intrinsics\n",
    "K0 = np.array(info[\"K0\"])\n",
    "K1 = np.array(info[\"K1\"])\n",
    "\n",
    "# Load pre-computed keypoints (you can also run keypoint detectors of your choice)\n",
    "matches_0_file = os.path.join(sample_path, info[\"matches_0_file\"])\n",
    "matches_1_file = os.path.join(sample_path, info[\"matches_1_file\"])\n",
    "mkpts0 = np.load(matches_0_file)\n",
    "mkpts1 = np.load(matches_1_file)\n",
    "\n",
    "# Load pre-computed depth maps (you can also run Monocular Depth models of your choice)\n",
    "depth_0_file = os.path.join(sample_path, info[\"depth_0_file\"])\n",
    "depth_1_file = os.path.join(sample_path, info[\"depth_1_file\"])\n",
    "depth_map0 = np.load(depth_0_file)\n",
    "depth_map1 = np.load(depth_pp1_file)\n",
    "\n",
    "# Query the depth priors of the keypoints\n",
    "depth0 = get_depths(image0, depth_map0, mkpts0)\n",
    "depth1 = get_depths(image1, depth_map1, mkpts1)\n",
    "\n",
    "# Compute the principal points\n",
    "pp0 = (np.array(image0.shape[:2][::-1]) - 1) / 2\n",
    "pp1 = (np.array(image1.shape[:2][::-1]) - 1) / 2\n",
    "pp = pp0\n",
    "\n",
    "# Run hybrid estimation\n",
    "pose, stats = madpose.HybridEstimatePoseScaleOffsetSharedFocal(\n",
    "    mkpts0,\n",
    "    mkpts1,\n",
    "    depth0,\n",
    "    depth1,\n",
    "    [depth_map0.min(), depth_map1.min()],\n",
    "    pp0,\n",
    "    pp1,\n",
    "    options,\n",
    "    est_config,\n",
    ")\n",
    "# rotation and translation of the estimated pose\n",
    "R_est, t_est = pose.R(), pose.t()\n",
    "# scale and offsets of the affine corrected depth maps\n",
    "s_est, o0_est, o1_est = pose.scale, pose.offset0, pose.offset1\n",
    "# estimated shared focal length\n",
    "f_est = pose.focal\n",
    "\n",
    "# Load the GT Pose\n",
    "T_0to1 = np.array(info[\"T_0to1\"])\n",
    "\n",
    "# Compute the pose error\n",
    "err_t, err_R = compute_pose_error(T_0to1, R_est, t_est)\n",
    "\n",
    "# Compute the focal error\n",
    "f_mean = np.mean([K0[0, 0], K0[1, 1], K1[0, 0], K1[1, 1]])\n",
    "err_f = np.abs(f_est - f_mean) / f_mean\n",
    "\n",
    "print(\"--- Hybrid Estimation Results ---\")\n",
    "print(f\"Rotation Error: {err_R:.4f} degrees\")\n",
    "print(f\"Translation Error: {err_t:.4f} degrees\")\n",
    "print(f\"Focal Error: {(err_f * 100):.2f}%\")\n",
    "print(f\"Estimated scale, offset0, offset1: {s_est:.4f}, {o0_est:.4f}, {o1_est:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18784f-0abe-4014-99ca-aa04a0229a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c0b45-9534-45c0-8205-4065e684fbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mkpts0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f90484-5e6a-4292-beef-c3eafd1e88c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 10:09:51.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mromatch.models.model_zoo.roma_models\u001b[0m:\u001b[36mroma_model\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mUsing coarse resolution (560, 560), and upsample res (864, 864)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Try importing the indoor specific model\n",
    "from romatch import roma_indoor\n",
    "\n",
    "# Initialize\n",
    "roma_model = roma_indoor(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8574d-7ff6-4955-b2ef-830ef21289a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path0=os.path.join(sample_path, \"image0.png\")\n",
    "img_path1=os.path.join(sample_path, \"image1.png\")\n",
    "warp, certainty = roma_model.match(img_path0, img_path1, device=\"cuda\")\n",
    "# Sample matches for estimation\n",
    "matches, certainty = roma_model.sample(warp, certainty)\n",
    "# Convert to pixel coordinates (RoMa produces matches in [-1,1]x[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870c194-64a2-432f-9f80-ca8adf4d15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "certainty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e23b0-31c4-4e26-b9ae-ddc5d14f1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def visualize_roma_matches(imA_path, imB_path, matches, roma_model):\n",
    "    # 1. Load images to get dimensions\n",
    "    imA = Image.open(imA_path)\n",
    "    imB = Image.open(imB_path)\n",
    "    W_A, H_A = imA.size\n",
    "    W_B, H_B = imB.size\n",
    "\n",
    "    # 2. Convert normalized matches [-1, 1] to pixel coordinates\n",
    "    # RoMa's to_pixel_coordinates handles the math for you\n",
    "    kptsA, kptsB = roma_model.to_pixel_coordinates(matches, H_A, W_A, H_B, W_B)\n",
    "\n",
    "    # Move to CPU/Numpy for plotting\n",
    "    kptsA = kptsA.cpu().numpy()\n",
    "    kptsB = kptsB.cpu().numpy()\n",
    "\n",
    "    # 3. Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # Concatenate images side-by-side\n",
    "    imA_np = np.array(imA)\n",
    "    imB_np = np.array(imB)\n",
    "    \n",
    "    # Handle different heights by padding (optional, but good for robustness)\n",
    "    hA, wA, c = imA_np.shape\n",
    "    hB, wB, c = imB_np.shape\n",
    "    max_h = max(hA, hB)\n",
    "    \n",
    "    # Create canvas\n",
    "    canvas = np.zeros((max_h, wA + wB, 3), dtype=np.uint8)\n",
    "    canvas[:hA, :wA, :] = imA_np\n",
    "    canvas[:hB, wA:wA+wB, :] = imB_np\n",
    "    \n",
    "    ax.imshow(canvas)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 4. Draw matches\n",
    "    # Shift kptsB x-coordinates by width of image A\n",
    "    kptsB_shifted = kptsB.copy()\n",
    "    kptsB_shifted[:, 0] += wA\n",
    "\n",
    "    # Plot lines\n",
    "    # Using a random subset if there are too many matches to see clearly\n",
    "    num_to_plot = min(len(kptsA), 500) \n",
    "    indices = np.random.choice(len(kptsA), num_to_plot, replace=False)\n",
    "\n",
    "    for i in indices:\n",
    "        # Draw line\n",
    "        ax.plot([kptsA[i, 0], kptsB_shifted[i, 0]], \n",
    "                [kptsA[i, 1], kptsB_shifted[i, 1]], \n",
    "                c='lime', linewidth=0.5, alpha=0.7)\n",
    "        \n",
    "        # Draw dots\n",
    "        ax.scatter(kptsA[i, 0], kptsA[i, 1], c='lime', s=5)\n",
    "        ax.scatter(kptsB_shifted[i, 0], kptsB_shifted[i, 1], c='lime', s=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- USAGE ---\n",
    "# Assuming you already ran:\n",
    "# matches, certainty = roma_model.sample(warp, certainty)\n",
    "\n",
    "visualize_roma_matches(img_path0, img_path1, matches, roma_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42befa03-888d-45fd-ae02-0e757ebdc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "imA = Image.open(img_path0)\n",
    "imB = Image.open(img_path1)\n",
    "W_A, H_A = imA.size\n",
    "W_B, H_B = imB.size\n",
    "\n",
    "# 2. Convert normalized matches [-1, 1] to pixel coordinates\n",
    "# RoMa's to_pixel_coordinates handles the math for you\n",
    "kptsA, kptsB = roma_model.to_pixel_coordinates(matches, H_A, W_A, H_B, W_B)\n",
    "\n",
    "# Move to CPU/Numpy for plotting\n",
    "kptsA = kptsA.cpu().numpy()\n",
    "kptsB = kptsB.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9266ea-f107-4374-9b33-3333c142d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab2e6f-2a26-42d1-941b-17c902316876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af432593-276c-4f5e-a074-af429c7f5bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
